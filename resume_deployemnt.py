# -*- coding: utf-8 -*-
"""Resume_deployemnt.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dpR-xRz9nQR678oc10h1r6oNUumqNLVX
"""

#importing libraries
import streamlit as st
from PyPDF2 import PdfReader
from docx import Document
import joblib
import pandas as pd
import docx2txt
import re
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
import string
from sklearn.feature_extraction.text import TfidfVectorizer

nltk.download('stopwords')

# Loading pickle files
model = joblib.load('model_RFC.pkl')
vectorizer = joblib.load('VECTOR.pkl')

# Read the PDF file and extract text from each page
def Read_pdf(file):
    pdfreader = PdfReader(file)
    count = len(pdfreader.pages)
    all_page_text = ""
    for i in range(count):
        page = pdfreader.pages[i]
        all_page_text += page.extract_text()
    return all_page_text

 # Read the DOCX file and extract text
def Read_docx(file):
    try:
        pdraw_text2 = docx2txt.process(file)
        return pdraw_text2
    except:
        st.error("")
        return ""

# Perform text preprocessing steps
def preprocess_text(text):
    if text is None:
        return ""
    text = text.lower()
    text = re.sub('http\S+\s*', ' ', text)
    text = re.sub('#\S+', '', text)
    text = re.sub(r'\d+', '', text)
    text = re.sub('@\S+', '  ', text)
    text = re.sub(r'\b\d+\b', '', text)
    text = re.sub('[%s]' % re.escape("""!"#$%&'()*+,-./:;<=>?@[\]^_`{|}~"""), ' ', text)
    text = re.sub(r'[^\x00-\x7f]', ' ', text)
    text = re.sub('\s+', ' ', text)
    text = text.translate(str.maketrans('', '', string.punctuation))
    tokens = word_tokenize(text)
    stop_words = set(stopwords.words('english'))
    tokens = [token for token in tokens if token not in stop_words]
    lemmatizer = WordNetLemmatizer()
    tokens = [lemmatizer.lemmatize(token) for token in tokens]
    preprocessed_text = ' '.join(tokens)
    return preprocessed_text

# Defining a function to process the uploaded file and make predictions
def process_file(file):
    text = ''
    file_extension = file.name.split('.')[-1]
    if file_extension == 'pdf':
        text = Read_pdf(file)
    elif file_extension in ['docx', 'doc']:
        text = Read_docx(file)
    else:
        st.error('Unsupported file format. Please upload a PDF or DOC/DOCX file.')
        return

    preprocessed_text = preprocess_text(text)
    vectorized_text = vectorizer.transform([preprocessed_text])
    prediction = model.predict(vectorized_text)
    return prediction[0]

# Creating the Streamlit app and defining its behavior
def main():
    st.title('Resume Classification')
    st.write('Upload a resume in PDF or DOC/DOCX format to classify.')

    upload_file = st.file_uploader('Upload Your Resumes', type=['docx', 'txt', 'doc', 'pdf'],
                                   accept_multiple_files=True)

    if upload_file is not None:
        data = []
        for file in upload_file:
            file_name = file.name
            file_type = file.type

            text = ""
            if file_type == 'application/pdf':
                text = Read_pdf(file)
            elif file_type == 'application/vnd.openxmlformats-officedocument.wordprocessingml.document':
                text = Read_docx(file)
            elif file_type == 'text/plain':
                text = file.read().decode('utf-8')

            prediction = process_file(file)
            data.append({'File Name': file_name, 'Prediction': prediction})

        df = pd.DataFrame(data)
        st.write(df)

if __name__ == '__main__':
    main()

